import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import os

# Loads training and validation datasets from CSV files.
# Splits the training data into features (`X`) and target (`y`), where `pm2.5` is the target variable.
# Standardizes feature values using `StandardScaler` to normalize data before model training.
# Defines multiple regression models (`LinearRegression`, `Ridge`, `Lasso`, `RandomForestRegressor`, `GradientBoostingRegressor`).
# Trains each model and evaluates performance using Mean Squared Error (MSE).
# Extracts feature importance
# Normalizes feature importance values across models and calculates mean importance for ranking.


input_folder = "plain_data"
train_file = os.path.join(input_folder, "train_data.csv")
validate_file = os.path.join(input_folder, "validate_data.csv")

train_data = pd.read_csv(train_file)
validate_data = pd.read_csv(validate_file)

def evaluate_feature_importance(train_data):
    X = train_data.drop(columns=['pm2.5'])
    y = train_data['pm2.5']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    models = {
        'LinearRegression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=0.1),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }

    feature_importances = pd.DataFrame(index=X.columns)

    for name, model in models.items():
        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        print(f"{name} MSE: {mse:.4f}")

        if hasattr(model, 'coef_'):
            feature_importances[name] = np.abs(model.coef_)
        elif hasattr(model, 'feature_importances_'):
            feature_importances[name] = model.feature_importances_

    feature_importances = feature_importances.div(feature_importances.sum(axis=0), axis=1)

    feature_importances['Mean_Importance'] = feature_importances.mean(axis=1)
    feature_importances = feature_importances.sort_values(by='Mean_Importance', ascending=False)

    return feature_importances

feature_importance = evaluate_feature_importance(train_data)

print(feature_importance)

output_file = os.path.join(input_folder, "feature_importance.csv")
feature_importance.to_csv(output_file)
